% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/check_URLs.R
\name{check_urls}
\alias{check_urls}
\title{Check URLs in Text for Validity}
\usage{
check_urls(text, tout = 5)
}
\arguments{
\item{text}{A character string containing potential URLs to be checked.}

\item{tout}{The timeout, defaults to 5 seconds}
}
\value{
A data frame with the following columns:
\item{url}{The extracted URLs from the text.}
\item{valid}{Logical value indicating whether each URL is valid.}
\item{status_code}{HTTP status code returned (if available).}
\item{error}{Error message if the URL is invalid, NA otherwise.}
}
\description{
This function searches text for URLs, extracts them, and checks whether they resolve
to valid websites. It handles both broken URLs (connection errors, HTTP errors) and
URLs that technically resolve but lead to "not found" pages.
}
\details{
The function first extracts all URLs from the input text using regular expressions,
then cleans them by removing trailing punctuation that might have been incorrectly
included during extraction. For each URL, it attempts an HTTP request and checks
both the status code and page content to determine validity.

URLs are considered invalid if:
\itemize{
\item The connection fails
\item They return HTTP error codes (4xx, 5xx)
\item They return a 2xx or 3xx status code but the page content suggests a "not found" page
}
}
\note{
The function requires the 'httr' and 'stringr' packages and will attempt to
install them if not present.
}
\examples{
\dontrun{
text <- "Check our website at https://example.com and https://invalid-url-example.xyz"
results <- check_urls(text)
print(results)

# Example with multiple URLs, some valid and some invalid
text2 <- "We deliver open data at https://osf.io/f5zjr/; more material is found at
          https://osf.io/f5zjl/ and https://osf.oi/asdf."
results2 <- check_urls(text2)
print(results2)
}

}
